{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2bb915-5667-4913-83f7-ccf35d282620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9000\n",
      "http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carrega o conteúdo do .env\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Usa as variáveis\n",
    "access_key = os.getenv('MINIO_ROOT_USER')\n",
    "secret_key = os.getenv('MINIO_ROOT_PASSWORD')\n",
    "endpoint_minio = os.getenv('MINIO_ROOT_ENDPOINT')\n",
    "endpoint_boto3 = os.getenv('MINIO_ROOT_BOTO3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf7f90b-2e6f-4053-94a3-d70c771edba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-62200ead-1d65-47c0-b372-fe7e82fd537c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-spark_2.12;3.3.1 in central\n",
      "\tfound io.delta#delta-storage;3.3.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (825ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.1/delta-spark_2.12-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.3.1!delta-spark_2.12.jar (1869ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (12290ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (330ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.1/delta-storage-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.3.1!delta-storage.jar (284ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (292ms)\n",
      ":: resolution report :: resolve 10151ms :: artifacts dl 15896ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.1 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   6   |   6   |   0   ||   6   |   6   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-62200ead-1d65-47c0-b372-fe7e82fd537c\n",
      "\tconfs: [default]\n",
      "\t6 artifacts copied, 0 already retrieved (282735kB/289ms)\n",
      "25/06/10 00:12:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo finalizado!\n"
     ]
    }
   ],
   "source": [
    "spark = ( \n",
    " SparkSession\n",
    " .builder\n",
    " .master('spark://spark-master:7077')\n",
    " .appName('MinIO')\n",
    " .config('spark.hadoop.fs.s3a.endpoint',endpoint_minio)\n",
    " .config('spark.hadoop.fs.s3a.access.key',access_key)\n",
    " .config('spark.hadoop.fs.s3a.secret.key',secret_key)\n",
    " .config('spark.hadoop.fs.s3a.path.style.access','true')\n",
    " .config('spark.hadoop.fs.s3a.connection.ssl.enabled','false')\n",
    " #.config('spark.hadoop.fs.s3a.impl','org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.3.4')\n",
    " .config('spark.jars.packages','io.delta:delta-spark_2.12:3.3.1')\n",
    " .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "print('Processo finalizado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e76d876-265b-4486-b82d-7aab9f814b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### INICIANDO LEITURA DOS DADOS DA CAMADA BRONZE (INGESTÃO) ###########\n",
      "Configurando boto3 para acessar MinIO...\n",
      "✅\n",
      "Lendo os arquivos CSV existentes na camada bronze...\n",
      "Arquivos: ['index_1.csv', 'index_2.csv']\n",
      "✅\n",
      "Iniciando a leitura dos arquivos...\n",
      "Lendo: s3a://bronze/index_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/python/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/python/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m caminho = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marquivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLendo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaminho\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m df_total = df_total.union(df)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marquivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m finalizado!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print('########### INICIANDO LEITURA DOS DADOS DA CAMADA BRONZE (INGESTÃO) ###########')\n",
    "\n",
    "print('Configurando boto3 para acessar MinIO...')\n",
    "#Configurando boto3 pra acessar o minio\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=endpoint_boto3,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "print('✅')\n",
    "\n",
    "#definindo bucket da primeira camada do medalhão, que será a de ingestão\n",
    "bucket = 'bronze'\n",
    "\n",
    "#lendo os arquivos csv dentro do bucket, para que sempre que chegue um novo arquivo, ele seja incluído no df_total\n",
    "print('Lendo os arquivos CSV existentes na camada bronze...')\n",
    "resposta = s3.list_objects_v2(Bucket=bucket)\n",
    "arquivos = [\n",
    "    obj['Key'] for obj in resposta.get('Contents', [])\n",
    "    if obj['Key'].endswith('.csv')\n",
    "]\n",
    "print(f'Arquivos: {arquivos}')\n",
    "print('✅')\n",
    "\n",
    "#Criando schema para o dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"cash_type\", StringType(), True),\n",
    "    StructField(\"card\", StringType(), True),\n",
    "    StructField(\"money\", DoubleType(), True),\n",
    "    StructField(\"coffee_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Criando um DataFrame vazio com esse schema\n",
    "df_total = spark.createDataFrame([], schema)\n",
    "\n",
    "#Efetuando iteração para leitura de cada arquivo no MinIO\n",
    "print('Iniciando a leitura dos arquivos...')\n",
    "for arquivo in arquivos:\n",
    "    caminho = f\"s3a://{bucket}/{arquivo}\"\n",
    "    print(f\"Lendo: {caminho}\")\n",
    "    df = spark.read.csv(caminho, header=True, schema=schema)\n",
    "    df_total = df_total.union(df)\n",
    "    print(f\"{arquivo} finalizado!\")\n",
    "    \n",
    "print('✅')\n",
    "#Mostra os dados combinados\n",
    "print('Plotando os dados em uma tabela...')\n",
    "df_total.show(2)\n",
    "print('✅ \\n \\n')\n",
    "\n",
    "print('########### INICIANDO INGESTÃO DOS DADOS NA CAMADA SILVER (DADOS AGRUPADOS) ########### \\n')\n",
    "\n",
    "path_silver = 's3a://silver'\n",
    "name_parquet_silver = 'coffe_sales_TESTE_v.parquet'\n",
    "\n",
    "print(f'Salvando df_total em {path_silver} ...')\n",
    "df_total.coalesce(1).write.mode('overwrite').parquet(f'{path_silver}/{name_parquet_silver}')\n",
    "print('✅')\n",
    "\n",
    "print('########### INICIANDO O CONSUMO DOS DADOS DA CAMADA SILVER (DADOS AGRUPADOS) ########### \\n')\n",
    "\n",
    "print(f'Iniciando leitura dos dados: {path_silver}/{name_parquet_silver} ...')\n",
    "\n",
    "df_coffe = spark.read.parquet(f'{path_silver}/{name_parquet_silver}')\n",
    "\n",
    "print('Plotando os dados em uma tabela...')\n",
    "df_coffe.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
